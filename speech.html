<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Speech Features - LITCoder</title>
    <link rel="stylesheet" href="styles/main.css">
    <link rel="stylesheet" href="styles/tutorials.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>
    <div class="container">
        <nav class="nav">
            <a href="index.html" class="nav-logo">LITCoder</a>
            <div class="nav-links">
                <a href="index.html">Home</a>
                <a href="tutorials.html">Tutorials</a>
                <a href="https://arxiv.org/abs/2509.09152" target="_blank">Paper</a>
                <a href="https://github.com/GT-LIT-Lab/litcoder_core" target="_blank">GitHub</a>
            </div>
        </nav>

        <header class="tutorial-header">
            <h1>Speech Features Tutorial</h1>
            <p>This tutorial shows how to train encoding models using speech features with the LeBel assembly. Speech features extract representations from audio using speech models.</p>
        </header>

        <section class="tutorial-section">
            <h2>Overview</h2>
            <p>Speech features capture acoustic and linguistic information from audio stimuli using speech recognition models like Whisper or HuBERT. These features can be highly predictive of brain activity during audio-based experiments.</p>
        </section>

        <section class="tutorial-section">
            <h2>Key Components</h2>
            <ul>
                <li><strong>Assembly</strong>: Pre-packaged LeBel assembly containing brain data and audio paths</li>
                <li><strong>Feature Extractor</strong>: SpeechFeatureExtractor using speech recognition models</li>
                <li><strong>Audio Processing</strong>: Chunking and resampling of audio files</li>
                <li><strong>Caching</strong>: Multi-layer activation caching for efficient training</li>
                <li><strong>Downsampler</strong>: Aligns audio-level features with brain data timing</li>
                <li><strong>Model</strong>: Ridge regression with nested cross-validation</li>
                <li><strong>Trainer</strong>: AbstractTrainer orchestrates the entire pipeline</li>
            </ul>
        </section>

        <section class="tutorial-section">
            <h2>Step-by-Step Tutorial</h2>

            <h3>1. Load the Assembly</h3>
            <pre class="code-block"><code class="language-python">from encoding.assembly.assembly_loader import load_assembly

# Load the pre-packaged LeBel assembly
assembly = load_assembly("assembly_lebel_uts03.pkl")</code></pre>

            <h3>2. Set Up Audio Paths</h3>
            <pre class="code-block"><code class="language-python">import os

# Set up audio paths for speech model
base_audio_path = "/path/to/your/audio/files"  # Replace with your audio path

for story_name in assembly.stories:
    # Assuming audio files are named like: story_name.wav
    audio_file_path = os.path.join(base_audio_path, f"{story_name}.wav")

    # Set the audio path for this story
    if hasattr(assembly, "story_data") and story_name in assembly.story_data:
        assembly.story_data[story_name].audio_path = audio_file_path
        print(f"Set audio path for {story_name}: {audio_file_path}")</code></pre>

            <h3>3. Create Speech Feature Extractor</h3>
            <pre class="code-block"><code class="language-python">from encoding.features.factory import FeatureExtractorFactory

extractor = FeatureExtractorFactory.create_extractor(
    modality="speech",
    model_name="openai/whisper-tiny",  # Can be changed to other models
    config={
        "model_name": "openai/whisper-tiny",
        "chunk_size": 0.1,  # seconds between chunk starts (stride)
        "context_size": 16.0,  # seconds of audio per window
        "layer": 3,  # Layer index to extract features from
        "pool": "last",  # Pooling method: 'last' or 'mean'
        "target_sample_rate": 16000,  # Target sample rate for audio
        "device": "cuda",  # Can be "cuda", "cpu"
    },
    cache_dir="cache_speech",
)</code></pre>

            <h3>4. Set Up Downsampler and Model</h3>
            <pre class="code-block"><code class="language-python">from encoding.downsample.downsampling import Downsampler
from encoding.models.nested_cv import NestedCVModel

downsampler = Downsampler()
model = NestedCVModel(model_name="ridge_regression")</code></pre>

            <h3>5. Configure Training Parameters</h3>
            <pre class="code-block"><code class="language-python"># FIR delays for hemodynamic response modeling
fir_delays = [1, 2, 3, 4]

# Trimming configuration for LeBel dataset
trimming_config = {
    "train_features_start": 10,
    "train_features_end": -5,
    "train_targets_start": 0,
    "train_targets_end": None,
    "test_features_start": 50,
    "test_features_end": -5,
    "test_targets_start": 40,
    "test_targets_end": None,
}

# No additional downsampling configuration needed
downsample_config = {}</code></pre>

            <h3>6. Create and Run Trainer</h3>
            <pre class="code-block"><code class="language-python">from encoding.trainer import AbstractTrainer

trainer = AbstractTrainer(
    assembly=assembly,
    feature_extractors=[extractor],
    downsampler=downsampler,
    model=model,
    fir_delays=fir_delays,
    trimming_config=trimming_config,
    use_train_test_split=True,
    logger_backend="wandb",
    wandb_project_name="lebel-speech-model",
    dataset_type="lebel",
    results_dir="results",
    layer_idx=3,  # Pass layer_idx to trainer
)

metrics = trainer.train()
print(f"Median correlation: {metrics.get('median_score', float('nan')):.4f}")</code></pre>
        </section>

        <section class="tutorial-section">
            <h2>Understanding Speech Features</h2>
            <p>Speech features are extracted by:</p>
            <ol>
                <li><strong>Audio Loading</strong>: Audio files are loaded and resampled to target sample rate</li>
                <li><strong>Chunking</strong>: Audio is divided into overlapping chunks for processing</li>
                <li><strong>Model Forward Pass</strong>: Each chunk is processed through the speech model</li>
                <li><strong>Feature Extraction</strong>: Features are extracted from the specified layer</li>
                <li><strong>Pooling</strong>: Features are pooled across time (last token or mean)</li>
                <li><strong>Caching</strong>: Multi-layer activations are cached for efficiency</li>
                <li><strong>Downsampling</strong>: Features are aligned with brain data timing</li>
            </ol>
        </section>

        <section class="tutorial-section">
            <h2>Key Parameters</h2>
            <ul>
                <li><code>modality</code>: "speech" - specifies the feature type</li>
                <li><code>model_name</code>: "openai/whisper-tiny" - speech model to use</li>
                <li><code>chunk_size</code>: 0.1 - seconds between chunk starts (stride)</li>
                <li><code>context_size</code>: 16.0 - seconds of audio per window</li>
                <li><code>layer</code>: 3 - which layer to extract features from</li>
                <li><code>pool</code>: "last" - pooling method ('last' or 'mean')</li>
                <li><code>target_sample_rate</code>: 16000 - target sample rate for audio</li>
                <li><code>device</code>: "cuda" - device to run the model on</li>
                <li><code>cache_dir</code>: "cache_speech" - directory for caching</li>
            </ul>
        </section>

        <section class="tutorial-section">
            <h2>Caching System</h2>
            <p>The speech extractor uses a sophisticated caching system:</p>
            <ol>
                <li><strong>Multi-layer caching</strong>: All layers are cached together</li>
                <li><strong>Lazy loading</strong>: Layers are loaded on-demand</li>
                <li><strong>Efficient storage</strong>: Compressed storage of activations</li>
                <li><strong>Cache validation</strong>: Ensures cached data matches parameters</li>
            </ol>
            <p>This makes it efficient to experiment with different layers without recomputing features.</p>
        </section>

        <section class="tutorial-section">
            <h2>Training Configuration</h2>
            <ul>
                <li><strong>fir_delays</strong>: [1, 2, 3, 4] - temporal delays for hemodynamic response</li>
                <li><strong>trimming_config</strong>: LeBel-specific trimming to avoid boundary effects</li>
                <li><strong>layer_idx</strong>: 3 - which layer to use for training</li>
            </ul>
        </section>

        <footer class="footer">
            <div>© 2025 LITCoder Project — Georgia Tech</div>
            <div style="font-size: 0.8rem; margin-top: 4px;">Authors: Taha Binhuraib, Ruimin Gao, Anna A. Ivanova</div>
        </footer>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
</body>
</html> 